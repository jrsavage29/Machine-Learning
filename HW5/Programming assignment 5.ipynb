{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Representation for Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment, we will experiment with distributed representations of words. We'll also see how such an embedding can be constructed by applying principal component analysis to a suitably transformed matrix of word co-occurrence probabilities. For computational reasons, we'll use the moderately sized **Brown corpus of present-day American English** for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accessing the Brown corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Brown corpus* is available as part of the Python Natural Language Toolkit (`nltk`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\jrsav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jrsav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import brown, stopwords\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus consists of 500 samples of text drawn from a wide range of sources. When these are concatenated, they form a very long stream of over a million words, which is available as `brown.words()`. Let's look at the first 50 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "Fulton\n",
      "County\n",
      "Grand\n",
      "Jury\n",
      "said\n",
      "Friday\n",
      "an\n",
      "investigation\n",
      "of\n",
      "Atlanta's\n",
      "recent\n",
      "primary\n",
      "election\n",
      "produced\n",
      "``\n",
      "no\n",
      "evidence\n",
      "''\n",
      "that\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print (brown.words()[i],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing anything else, let's remove stopwords and punctuation and make everything lowercase. The resulting sequence will be stored in `my_word_stream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = set(stopwords.words('english'))\n",
    "word_stream = [str(w).lower() for w in brown.words() if w.lower() not in my_stopwords]\n",
    "my_word_stream = [w for w in word_stream if (len(w) > 1 and w.isalnum())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the initial 20 words in `my_word_stream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fulton',\n",
       " 'county',\n",
       " 'grand',\n",
       " 'jury',\n",
       " 'said',\n",
       " 'friday',\n",
       " 'investigation',\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " 'evidence',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " 'jury',\n",
       " 'said',\n",
       " 'presentments',\n",
       " 'city',\n",
       " 'executive']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_word_stream[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computing co-occurrence probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P1**: Complete the following code to get a list of words and their counts. Report how many times does the word \"evidence\" and \"investigation\" appears in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(my_word_stream)\n",
    "# print(N)\n",
    "words = []\n",
    "totals = {}\n",
    "\n",
    "## STUDENT: Your code here\n",
    "# words: a python list of unique words in the document my_word_stream as the vocabulary\n",
    "# totals: a python dictionary, where each word is a key, and the corresponding value\n",
    "#         is the number of times this word appears in the document my_word_stream\n",
    "\n",
    "words = my_word_stream\n",
    "\n",
    "for word in words:\n",
    "    if word in totals:\n",
    "        totals[word] += 1\n",
    "    else:\n",
    "        totals[word] = 1\n",
    "    \n",
    "    \n",
    "## STUDENT CODE ENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \" evidence \" appears  204  times\n",
      "Word \" investigation \" appears  51  times\n"
     ]
    }
   ],
   "source": [
    "## STUDENT: Report how many times does the word \"evidence\" and \"investigation\" appears in the corpus.\n",
    "print('Word \"',words[11],'\" appears ',totals[words[11]], ' times')\n",
    "print('Word \"',words[6],'\" appears ',totals[words[6]], ' times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task P2**: Decide on the vocabulary. There are two potentially distinct vocabularies: the words for which we will obtain embeddings (`vocab_words`) and the words we will consider when looking at context information (`context_words`). We will take the former to be all words that occur at least 20 times, and the latter to be all words that occur at least 100 times. We will stick to these choices for this assignment, but feel free to play around with them and find something better.\n",
    "\n",
    "How large are these two word lists? Note down these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabulary words  4720 ;\n",
      "Number of context words  918 ;\n"
     ]
    }
   ],
   "source": [
    "## STUDENT: Your code here\n",
    "\n",
    "vocab_words = [] # a list of words whose occurances (totals) are > 19\n",
    "context_words = [] # a list of words whose occurances (totals) are > 99\n",
    "\n",
    "wordList = totals.items()\n",
    "for word in wordList:\n",
    "    if word[1] > 99:\n",
    "        context_words.append(word[0])\n",
    "    if word[1] > 19:\n",
    "        vocab_words.append(word[0])\n",
    "        \n",
    "## STUDENT CODE ENDS\n",
    "print ('Number of vocabulary words ',len(vocab_words), ';')\n",
    "print ('Number of context words ',len(context_words), ';' )\n",
    "#print(context_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P3**: Get co-occurrence counts. These are defined as follows, for a small constant `window_size=2`.\n",
    "\n",
    "* Let `w0` be any word in `vocab_words` and `w` any word in `context_words`.\n",
    "* Each time `w0` occurs in the corpus, look at the window of `window_size` words before and after it. If `w` appears in this window, we say it appears in the context of (this particular occurrence of) `w0`.\n",
    "* Define `counts[w0][w]` as the total number of times `w` occurs in the context of `w0`.\n",
    "\n",
    "Complete the function `get_counts`, which computes the `counts` array and returns it as a dictionary (of dictionaries). Find how many times the word \"fact\" appears in the context of ”evidence\" with window_size=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(window_size=2):\n",
    "    ## Input:\n",
    "    #  window_size: for each word w0, its context includes window_size words before and after it. \n",
    "    #  For instance, if window_size = 2, it means we look at w1 w2 w0 w3 w4, where  w1, w2, w3, w4 are \n",
    "    #  context woreds\n",
    "    ## Output:\n",
    "    #  counts: a python dictionary (of dictionaries) where counts[w0][w] indicate the number of times the word w appears \n",
    "    #  in the context of w0 (Note: counts[w0] is also a python dictionary)\n",
    "    \n",
    "    counts = {}\n",
    "    for w0 in vocab_words:\n",
    "        counts[w0] = {} \n",
    "        \n",
    "    ## STUDENT: Your code here\n",
    "    \n",
    "    #set each vocab word to have counter for all possible context words\n",
    "    for w0 in vocab_words:\n",
    "        for word in context_words:\n",
    "            counts[w0][word]= 0\n",
    "        \n",
    "    vocab_list = list(counts.keys())\n",
    "    words = my_word_stream\n",
    "    index_list = []\n",
    "    window_list = []\n",
    "    vocab_index = 0\n",
    "    indexMin = 0\n",
    "    indexMax = 0\n",
    "    word_of_interest = \"\"\n",
    "    \n",
    "    #iterate through the vocab words\n",
    "    for i in range(len(vocab_list)):\n",
    "        \n",
    "        #get the current vocab_word\n",
    "        word_of_interest = vocab_list[i]\n",
    "        \n",
    "        #find the index of every occurence of the current vocab word in the corpus\n",
    "        index_list = [i for i,word in enumerate(words) if word == word_of_interest]\n",
    "        \n",
    "        #print(index_list)\n",
    "        #go to every occurence of the vocab word in the corpus and check the window size\n",
    "        for i in index_list:\n",
    "\n",
    "            indexMin = i - window_size\n",
    "            indexMax = i + window_size + 1\n",
    "\n",
    "            if indexMin < 0:\n",
    "                indexMin = 0\n",
    "\n",
    "            if indexMax > len(words) - 1:\n",
    "                indexMax = len(words)\n",
    "\n",
    "            window_list = words[indexMin:indexMax]\n",
    "            window_list.remove(word_of_interest)\n",
    "            #print(window_list)\n",
    "\n",
    "            for word in context_words:\n",
    "                if word in window_list:\n",
    "                    counts[word_of_interest][word] += 1\n",
    "        \n",
    "    ## End of codes\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "## STUDENT: Report how many times the word \"fact\" appears in the context of ”evidence\".\n",
    "counts = get_counts(window_size=2)\n",
    "print (counts['evidence']['fact'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `probs[w0][]` to be the distribution over the context of `w0`, that is:\n",
    "* `probs[w0][w] = counts[w0][w] / (sum of all counts[w0][])`\n",
    "\n",
    "**Task P4**: Finish the function `get_co_occurrence_dictionary` that computes `probs`. Find the probability that the word \"fact\" appears in the context of ”evidence\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_co_occurrence_dictionary(counts):\n",
    "    ## Input:\n",
    "    #  counts: a python dictionary (of dictionaries) where counts[w0][w] indicate the number of times the word w appears \n",
    "    #  in the context of w0 (Note: counts[w0] is also a python dictionary)\n",
    "    ## Output:\n",
    "    #  probs: a python dictionary (of dictionaries) where probs[w0][w] indicate the probability that word w appears \n",
    "    #  in the context of word w0\n",
    "    \n",
    "    probs = {}\n",
    "    \n",
    "    ## STUDENT: Your code here\n",
    "    #initialize probability dictionary to have each vocab word have a probability of every context word be initially zero\n",
    "    for w0 in vocab_words:\n",
    "        probs[w0] = {} \n",
    "        for word in context_words:\n",
    "            probs[w0][word]= 0\n",
    "        \n",
    "    for w0 in list(probs.keys()):\n",
    "        for w in list(probs[w0].keys()):\n",
    "            probs[w0][w] = counts[w0][w]/sum( list( counts[w0].values() ) )\n",
    "            \n",
    "    ## End of codes\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010723860589812333\n"
     ]
    }
   ],
   "source": [
    "## STUDENT: Report how many times the word \"fact\" appears in the context of ”evidence\".\n",
    "probs = get_co_occurrence_dictionary(counts)\n",
    "print (probs['evidence']['fact'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final piece of information we need is the frequency of different context words. The function below, `get_context_word_distribution`, takes `counts` as input and returns (again, in dictionary form) the array:\n",
    "\n",
    "* `context_frequency[w]` = sum of all `counts[][w]` / sum of all `counts[][]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_word_distribution(counts):\n",
    "    counts_context = {}\n",
    "    sum_context = 0\n",
    "    context_frequency = {}\n",
    "    for w in context_words:\n",
    "        counts_context[w] = 0\n",
    "    for w0 in counts.keys():\n",
    "        for w in counts[w0].keys():\n",
    "            counts_context[w] = counts_context[w] + counts[w0][w]\n",
    "            sum_context = sum_context + counts[w0][w]\n",
    "    for w in context_words:\n",
    "        context_frequency[w] = float(counts_context[w])/float(sum_context)\n",
    "    return context_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P5**: Based on the various pieces of information above, we compute the **pointwise mutual information matrix**:\n",
    "* `PMI[i,j] = MAX(0, log probs[ith vocab word][jth context word] - log context_frequency[jth context word])`\n",
    "\n",
    "Complete the code to compute PMI for every word i and context word j. Report the output of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing counts and distributions\n",
      "Computing pointwise mutual information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-c9b5bb3dc9ab>:15: RuntimeWarning: divide by zero encountered in log\n",
      "  pmi[i,j] = max( 0, np.log(probs[w0][context_words[j]] ) - np.log(context_frequency[context_words[j]]) )\n"
     ]
    }
   ],
   "source": [
    "print (\"Computing counts and distributions\")\n",
    "counts = get_counts(2)\n",
    "probs = get_co_occurrence_dictionary(counts)\n",
    "context_frequency = get_context_word_distribution(counts)\n",
    "#\n",
    "print (\"Computing pointwise mutual information\")\n",
    "n_vocab = len(vocab_words)\n",
    "n_context = len(context_words)\n",
    "pmi = np.zeros((n_vocab, n_context))\n",
    "for i in range(0, n_vocab):\n",
    "    w0 = vocab_words[i]\n",
    "    for w in probs[w0].keys():\n",
    "        j = context_words.index(w)\n",
    "        ## STUDENT: Your code here\n",
    "        pmi[i,j] = max( 0, np.log(probs[w0][context_words[j]] ) - np.log(context_frequency[context_words[j]]) )\n",
    "        ## Student end of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6800791816244818\n"
     ]
    }
   ],
   "source": [
    "# STUDENT: report the following number\n",
    "print(pmi[vocab_words.index('evidence'),context_words.index('fact')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding of any word can then be taken as the corresponding row of this matrix. However, to reduce the dimension, we will apply principal component analysis (PCA).\n",
    "\n",
    "See this nice tutorial on PCA: https://www.youtube.com/watch?v=fkf4IBRSeEc\n",
    "\n",
    "Now reduce the dimension of the PMI vectors using principal component analysis. Here we bring it down to 100 dimensions, and then normalize the vectors to unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "vecs = pca.fit_transform(pmi)\n",
    "for i in range(0,n_vocab):\n",
    "    vecs[i] = vecs[i]/np.linalg.norm(vecs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to save this embedding so that it doesn't need to be computed every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = open(\"embedding.pickle\", \"wb\")\n",
    "pickle.dump(vocab_words, fd)\n",
    "pickle.dump(context_words, fd)\n",
    "pickle.dump(vecs, fd)\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimenting with the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get some insight into the embedding by looking at some intersting use cases.\n",
    "\n",
    "** Task P6**: Implement the following function that finds the nearest neighbor of a given word in the embedded space. Note down the answers to the following queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_NN(w,vecs,vocab_words,context_words):\n",
    "    ## Input:\n",
    "    #  w: word w\n",
    "    #  vecs: the embedding of words, as computed above\n",
    "    #  vocab_words: vocabulary words, as computed in Task P2\n",
    "    #  context_words: context words, as computed in Task P2\n",
    "    ## Output:\n",
    "    #  the nearest neighbor (word) to word w\n",
    "    if not(w in vocab_words):\n",
    "        print (\"Unknown word\")\n",
    "        return\n",
    "    \n",
    "    ## Student: your code here\n",
    "    vect = vecs[vocab_words.index(w)]\n",
    "    neighbor = 0\n",
    "    curr_dist = np.linalg.norm(vect - vecs[0])\n",
    "    for i in range(1, len(vocab_words)):\n",
    "        dist = np.linalg.norm(vect - vecs[i])\n",
    "        if(dist < curr_dist) and (dist > 0.0):\n",
    "            neighbor = i\n",
    "            curr_dist = dist\n",
    "    \n",
    "    return vocab_words[neighbor]\n",
    "    ## Student: code ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nations'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_NN('world',vecs,vocab_words,context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'freedom'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_NN('learning',vecs,vocab_words,context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'studies'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_NN('technology',vecs,vocab_words,context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woman'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_NN('man',vecs,vocab_words,context_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Task P7**: Implement the function that aims to solve the analogy problem:\n",
    "A is to B as C is to ?\n",
    "For example, A=King, B=Queen, C=man, and the answer for ? should be ideally woman (you will see that this may not be  the case using the distributed representation).\n",
    "\n",
    "Finds the K-nearest neighbor of a given word in the embedded space. Note: instead of outputing only the nearest neighbor, you should find the K=10 nearest neighbors and see whether there is one in the list that makes sense. You should also exclude the words C in the output list.\n",
    "\n",
    "Also report another set A, B, C and the corresponding answer output by your problem. See if it makes sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogy(A,B,C,vecs,vocab_words,context_words):\n",
    "    ## Input:\n",
    "    #  A, B, C: words A, B, C\n",
    "    #  vecs: the embedding of words, as computed above\n",
    "    #  vocab_words: vocabulary words, as computed in Task P2\n",
    "    #  context_words: context words, as computed in Task P2\n",
    "    ## Output:\n",
    "    #  the word that solves the analogy problem\n",
    "    ## STUDENT: Your code \n",
    "    import operator\n",
    "    query_word = vecs[vocab_words.index(C)] + vecs[vocab_words.index(B)] - vecs[vocab_words.index(A)]\n",
    "    distances = []\n",
    "    for index in range(1, len(vocab_words)):\n",
    "        dist = np.linalg.norm(query_word - vecs[index])\n",
    "        \n",
    "        if(vocab_words[index] != A) and (vocab_words[index] != B) and (vocab_words[index] != C):\n",
    "            distances.append([index, dist, vocab_words[index]])\n",
    "            \n",
    "    distances.sort(key=operator.itemgetter(1))\n",
    "    \n",
    "    neighbors = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        neighbors.append(distances[i][0])\n",
    "    \n",
    "    output_vals = []\n",
    "    for i in range(len(neighbors)):\n",
    "        output_vals.append(vocab_words[neighbors[i]])\n",
    "    \n",
    "    return output_vals\n",
    "    ## STUDENT: your code ends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soul',\n",
       " 'kate',\n",
       " 'spoke',\n",
       " 'dying',\n",
       " 'wonder',\n",
       " 'loved',\n",
       " 'creatures',\n",
       " 'blame',\n",
       " 'gentle',\n",
       " 'old']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_analogy('king','queen','man',vecs,vocab_words,context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dark',\n",
       " 'closed',\n",
       " 'blue',\n",
       " 'rain',\n",
       " 'saw',\n",
       " 'around',\n",
       " 'suddenly',\n",
       " 'hung',\n",
       " 'stood',\n",
       " 'rose']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_analogy('soil','grass','sun',vecs,vocab_words,context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['local',\n",
       " 'report',\n",
       " 'members',\n",
       " 'state',\n",
       " 'department',\n",
       " 'service',\n",
       " 'government',\n",
       " 'board',\n",
       " 'today',\n",
       " 'kennedy']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_analogy('mayor','city','president',vecs,vocab_words,context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
